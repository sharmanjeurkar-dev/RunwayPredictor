{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6308266,"sourceType":"datasetVersion","datasetId":3629391}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Look at images\nfrom pathlib import Path\nfrom PIL import Image\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom mpmath.identification import transforms\nfrom torchvision.ops.misc import interpolate\n\nrandom.seed(42)\nLOCALPATH = Path('data/1920x1080/1920x1080')\nKAGGLEPATH = Path('/kaggle/input/fs2020-runway-dataset/1920x1080/1920x1080')\n\nif os.path.exists(KAGGLEPATH):\n    target_dir_feature = KAGGLEPATH\nelse:\n    target_dir_feature = LOCALPATH\n\nif target_dir_feature.exists():\n    train_dir = target_dir_feature/'train'\n    test_dir = target_dir_feature/'test'\n    image_path_feature_list = list(train_dir.glob('*.png'))\n\n    if len(image_path_feature_list)>0:\n\n        random_image_path = random.choice(image_path_feature_list)\n        image = Image.open(random_image_path)\n\n        plt.figure(figsize=(15,15))\n        plt.imshow(image)\n        plt.title(random_image_path)\n        plt.axis('off')\n        plt.show()\n\n    else:\n        print(f'No images found in  {train_dir}')\n\nelse:\n    print('Directory not found')","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:45.218571232Z","start_time":"2026-01-27T04:35:39.959566634Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:29.294304Z","iopub.execute_input":"2026-01-30T10:46:29.294879Z","iopub.status.idle":"2026-01-30T10:46:36.992074Z","shell.execute_reply.started":"2026-01-30T10:46:29.294851Z","shell.execute_reply":"2026-01-30T10:46:36.991351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#look at the label masks\nLOACLPATHMASKS = Path('data/labels/labels/areas/')\nKAGGLEPATHMASKS = Path('/kaggle/input/fs2020-runway-dataset/labels/labels/areas/')\nrandom.seed(42)\n\nif os.path.exists(KAGGLEPATHMASKS):\n    target_dir_masks = KAGGLEPATHMASKS\nelse:\n    target_dir_masks = LOACLPATHMASKS\n\nif target_dir_masks.exists():\n    train_masks_dir = target_dir_masks/'train_labels_1920x1080/'\n    test_masks_dir = target_dir_masks/'test_labels_1920x1080/'\n\n    image_mask_list = list(train_masks_dir.glob('*.png'))\n    if len(image_mask_list)>0:\n        random_image_mask_path = random.choice(image_mask_list)\n        image_mask = Image.open(random_image_mask_path)\n\n        plt.figure(figsize=(15,15))\n        plt.imshow(image_mask)\n        plt.title(random_image_mask_path)\n        plt.axis('off')\n        plt.show()\n\n    else:\n        print(f'No images found in  {train_dir_labels}')\nelse:\n    print('Directory not found')","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:45.698168771Z","start_time":"2026-01-27T04:35:45.235709285Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:36.993303Z","iopub.execute_input":"2026-01-30T10:46:36.993627Z","iopub.status.idle":"2026-01-30T10:46:37.43505Z","shell.execute_reply.started":"2026-01-30T10:46:36.993605Z","shell.execute_reply":"2026-01-30T10:46:37.434307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:37.436035Z","iopub.execute_input":"2026-01-30T10:46:37.436337Z","iopub.status.idle":"2026-01-30T10:46:37.440275Z","shell.execute_reply.started":"2026-01-30T10:46:37.436307Z","shell.execute_reply":"2026-01-30T10:46:37.439503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:37.441948Z","iopub.execute_input":"2026-01-30T10:46:37.442234Z","iopub.status.idle":"2026-01-30T10:46:37.451062Z","shell.execute_reply.started":"2026-01-30T10:46:37.442211Z","shell.execute_reply":"2026-01-30T10:46:37.45042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#set device agnostics\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:45.832969109Z","start_time":"2026-01-27T04:35:45.706908859Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:37.451962Z","iopub.execute_input":"2026-01-30T10:46:37.452252Z","iopub.status.idle":"2026-01-30T10:46:37.711345Z","shell.execute_reply.started":"2026-01-30T10:46:37.452204Z","shell.execute_reply":"2026-01-30T10:46:37.710599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nmask_dir = r\"/kaggle/input/fs2020-runway-dataset/labels/labels/areas/train_labels_1920x1080\"\ntest_mask_dir = r\"/kaggle/input/fs2020-runway-dataset/labels/labels/areas/test_labels_1920x1080\"\nbinary_mask_dir = r\"/kaggle/working/binary_mask\"\nbinary_test_mask_dir = r\"/kaggle/working/binary_test_mask\"\n\ndef convert2Binary(mask_dir, binary_mask_dir):\n    os.makedirs(binary_mask_dir, exist_ok = True)\n    for filename in os.listdir(mask_dir):\n        if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            continue\n        img_path = os.path.join(mask_dir, filename)\n        img = cv2.imread(img_path)\n    \n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n        binary = np.where(gray > 0, 255, 0).astype(np.uint8)\n    \n        cv2.imwrite(os.path.join(binary_mask_dir, filename), binary)\n\nconvert2Binary(test_mask_dir, binary_test_mask_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:46:37.712425Z","iopub.execute_input":"2026-01-30T10:46:37.71271Z","iopub.status.idle":"2026-01-30T10:47:26.927141Z","shell.execute_reply.started":"2026-01-30T10:46:37.712688Z","shell.execute_reply":"2026-01-30T10:47:26.926243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"convert2Binary(mask_dir, binary_mask_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:47:26.928184Z","iopub.execute_input":"2026-01-30T10:47:26.928462Z","iopub.status.idle":"2026-01-30T10:49:23.609153Z","shell.execute_reply.started":"2026-01-30T10:47:26.928434Z","shell.execute_reply":"2026-01-30T10:49:23.608551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TRransforms images as well masks\nfrom torchvision import transforms\n\nimage_transform = transforms.Compose([\n    transforms.Resize((288,512)),\n    transforms.ColorJitter(brightness=[max(0, 1 - 0.2), 1 + 0.2],\n                           contrast=[max(0, 1 - 0.2), 1 + 0.2],\n                           saturation=[max(0, 1 - 0.2), 1 + 0.2],\n                           hue=0.02),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n\n])\n\nmask_transform = transforms.Compose([\n    transforms.Resize((288,512),interpolation=transforms.InterpolationMode.NEAREST),\n    #transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n])","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:53:54.496500291Z","start_time":"2026-01-27T04:53:54.460750577Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:23.609997Z","iopub.execute_input":"2026-01-30T10:49:23.61026Z","iopub.status.idle":"2026-01-30T10:49:23.615684Z","shell.execute_reply.started":"2026-01-30T10:49:23.610235Z","shell.execute_reply":"2026-01-30T10:49:23.615069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#View Transformed images\nrandom.seed(42)\n\nrandom_image_path = random.sample(image_path_feature_list,3)\n\nfor image_path in random_image_path:\n    with Image.open(image_path) as img:\n        fig,ax = plt.subplots(1,2, figsize=(15,15))\n\n        ax[0].imshow(img)\n        ax[0].set_title('Original Image')\n        ax[0].axis('off')\n\n        transformed_img = image_transform(img).permute(1, 2, 0)\n\n        ax[1].imshow(transformed_img)\n        ax[1].set_title('Transformed Image')\n        ax[1].axis('off')\n","metadata":{"ExecuteTime":{"end_time":"2026-01-27T05:29:22.277598233Z","start_time":"2026-01-27T05:29:20.739952819Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:23.616524Z","iopub.execute_input":"2026-01-30T10:49:23.616775Z","iopub.status.idle":"2026-01-30T10:49:25.14586Z","shell.execute_reply.started":"2026-01-30T10:49:23.616743Z","shell.execute_reply":"2026-01-30T10:49:25.145142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#View Transformed labels\nrandom.seed(42)\n\nrandom_mask_path = random.sample(image_mask_list,3)\n\nfor image_path in random_mask_path:\n    with Image.open(image_path) as img:\n        fig,ax = plt.subplots(1,2, figsize=(15,15))\n\n        ax[0].imshow(img)\n        ax[0].set_title('Original Image')\n        ax[0].axis('off')\n\n        transformed_img = mask_transform(img).permute(1, 2, 0)\n\n        ax[1].imshow(transformed_img)\n        ax[1].set_title('Transformed Image')\n        ax[1].axis('off')\n","metadata":{"ExecuteTime":{"end_time":"2026-01-27T05:29:13.686086446Z","start_time":"2026-01-27T05:29:12.276761606Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:25.147955Z","iopub.execute_input":"2026-01-30T10:49:25.14823Z","iopub.status.idle":"2026-01-30T10:49:26.278136Z","shell.execute_reply.started":"2026-01-30T10:49:25.148208Z","shell.execute_reply":"2026-01-30T10:49:26.277524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass RunwayDataset(Dataset):\n    def __init__(self, train_dir,mask_dir, image_transform=None, mask_transform=None):\n        self.train_dir = Path(train_dir)\n        self.image_transform = image_transform\n        self.mask_dir = Path(mask_dir)\n        self.mask_transform = mask_transform\n\n\n        self.image_paths = sorted(list(self.train_dir.glob(\"*.jpg\")) + list(self.train_dir.glob(\"*.png\")))\n\n\n        self.mask_paths = sorted(list(self.mask_dir.glob(\"*.png\")))\n\n\n        if len(self.image_paths) != len(self.mask_paths):\n            print(f\"⚠️ Warning: Found {len(self.image_paths)} images but {len(self.mask_paths)} masks!\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n\n        img_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n\n        image = Image.open(img_path)\n\n        mask = Image.open(mask_path).convert(\"L\")\n\n\n        if self.image_transform:\n            image = self.image_transform(image)\n\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n\n        return image, mask","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:50.204056625Z","start_time":"2026-01-27T04:35:49.976272021Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.278966Z","iopub.execute_input":"2026-01-30T10:49:26.279268Z","iopub.status.idle":"2026-01-30T10:49:26.286372Z","shell.execute_reply.started":"2026-01-30T10:49:26.279232Z","shell.execute_reply":"2026-01-30T10:49:26.285627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = RunwayDataset(\n    train_dir = train_dir,\n    mask_dir = binary_mask_dir,\n    image_transform = image_transform,\n    mask_transform= mask_transform\n)\n\ntest_dataset = RunwayDataset(\n    train_dir =test_dir,\n    mask_dir = binary_test_mask_dir,\n    image_transform = image_transform,\n    mask_transform= mask_transform\n)\ntrain_dataset.__getitem__(1)","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:51.115281048Z","start_time":"2026-01-27T04:35:50.206841434Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.287269Z","iopub.execute_input":"2026-01-30T10:49:26.287502Z","iopub.status.idle":"2026-01-30T10:49:26.589786Z","shell.execute_reply.started":"2026-01-30T10:49:26.287481Z","shell.execute_reply":"2026-01-30T10:49:26.58904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load Data\nfrom torch.utils.data import DataLoader\n\ntrain_data_load = DataLoader(\n    train_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = True,\n    pin_memory=True,\n    drop_last=True\n)\ntest_data_load = DataLoader(\n    test_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = True\n)","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:51.271136825Z","start_time":"2026-01-27T04:35:51.161579467Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.590688Z","iopub.execute_input":"2026-01-30T10:49:26.590962Z","iopub.status.idle":"2026-01-30T10:49:26.595262Z","shell.execute_reply.started":"2026-01-30T10:49:26.590941Z","shell.execute_reply":"2026-01-30T10:49:26.594599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n#DoubleConvLayer\nclass DoubleConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n\n        )\n    def forward(self,x):\n        return self.double_conv(x)\n","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:37:30.553083481Z","start_time":"2026-01-27T04:37:30.519765359Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.596861Z","iopub.execute_input":"2026-01-30T10:49:26.597087Z","iopub.status.idle":"2026-01-30T10:49:26.608602Z","shell.execute_reply.started":"2026-01-30T10:49:26.597051Z","shell.execute_reply":"2026-01-30T10:49:26.607915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Downgrade resolution\nclass Down(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.max_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            DoubleConv2d(in_channels, out_channels)\n        )\n    def forward(self,x):\n        return self.max_pool(x)","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:39:09.511977362Z","start_time":"2026-01-27T04:39:09.452452473Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.609337Z","iopub.execute_input":"2026-01-30T10:49:26.609629Z","iopub.status.idle":"2026-01-30T10:49:26.622503Z","shell.execute_reply.started":"2026-01-30T10:49:26.6096Z","shell.execute_reply":"2026-01-30T10:49:26.621903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Upsacle image and merge with results of same level downscaled images\nfrom torch.nn import functional as F\nclass Up(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n        self.conv = DoubleConv2d(in_channels, out_channels)\n\n    def forward(self,x1,x2):\n        x1 = self.up(x1)\n\n        diffY = x2.size()[2]  - x1.size()[2]\n        diffX = x2.size()[3]  - x1.size()[3]\n\n        x1 = F.pad(x1,[diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        x = torch.cat((x1,x2),dim=1)\n        return self.conv(x)\n","metadata":{"ExecuteTime":{"end_time":"2026-01-27T06:14:48.934462090Z","start_time":"2026-01-27T06:14:48.911601334Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.623244Z","iopub.execute_input":"2026-01-30T10:49:26.623509Z","iopub.status.idle":"2026-01-30T10:49:26.634046Z","shell.execute_reply.started":"2026-01-30T10:49:26.623477Z","shell.execute_reply":"2026-01-30T10:49:26.633404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#feature mapping\nclass Out(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(Out,self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self,x):\n        return self.conv(x)","metadata":{"ExecuteTime":{"end_time":"2026-01-27T04:35:51.978392283Z","start_time":"2026-01-27T04:35:51.867929717Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.634884Z","iopub.execute_input":"2026-01-30T10:49:26.635473Z","iopub.status.idle":"2026-01-30T10:49:26.645975Z","shell.execute_reply.started":"2026-01-30T10:49:26.635452Z","shell.execute_reply":"2026-01-30T10:49:26.645414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Main model class --brain\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n\n\n        self.inc = DoubleConv2d(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n\n\n        self.down4 = Down(512, 1024)\n\n\n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n\n\n        self.outc = Out(64, n_classes)\n\n    def forward(self, x):\n\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        # 2. Going Up (Concatenate with saved outputs)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n\n        # 3. Final Prediction\n        logits = self.outc(x)\n        return logits","metadata":{"ExecuteTime":{"end_time":"2026-01-27T06:11:39.148638013Z","start_time":"2026-01-27T06:11:39.018559165Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.646825Z","iopub.execute_input":"2026-01-30T10:49:26.647108Z","iopub.status.idle":"2026-01-30T10:49:26.657019Z","shell.execute_reply.started":"2026-01-30T10:49:26.647079Z","shell.execute_reply":"2026-01-30T10:49:26.65641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test\nmodelu  = UNet(n_channels=3, n_classes=1).to(device)\nmodelu","metadata":{"ExecuteTime":{"end_time":"2026-01-27T05:19:41.689218221Z","start_time":"2026-01-27T05:19:41.420398670Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:26.657695Z","iopub.execute_input":"2026-01-30T10:49:26.657897Z","iopub.status.idle":"2026-01-30T10:49:27.182758Z","shell.execute_reply.started":"2026-01-30T10:49:26.657868Z","shell.execute_reply":"2026-01-30T10:49:27.182028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this code to calculate the real class imbalance\ntotal_pixels = 0\nrunway_pixels = 0\n\nfor _, mask in train_data_load:\n    total_pixels += mask.numel()\n    runway_pixels += mask.sum().item()\n\nbackground_pixels = total_pixels - runway_pixels\nweight = background_pixels / runway_pixels\n\nprint(f\"Calculated weight: {weight:.2f}\")\npos_weight = torch.tensor([weight]).to(device)\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:49:27.183666Z","iopub.execute_input":"2026-01-30T10:49:27.183921Z","iopub.status.idle":"2026-01-30T10:51:19.363047Z","shell.execute_reply.started":"2026-01-30T10:49:27.1839Z","shell.execute_reply":"2026-01-30T10:51:19.362326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class DiceLoss(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n    \n#     def forward(self, pred, target):\n#         pred = torch.sigmoid(pred)\n#         smooth = 1.0\n        \n#         pred_flat = pred.view(-1)\n#         target_flat = target.view(-1)\n        \n#         intersection = (pred_flat * target_flat).sum()\n        \n#         return 1 - ((2. * intersection + smooth) /\n#                     (pred_flat.sum() + target_flat.sum() + smooth))\n\n# Use it:\n#loss_fn = DiceLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:51:19.364509Z","iopub.execute_input":"2026-01-30T10:51:19.364849Z","iopub.status.idle":"2026-01-30T10:51:19.368599Z","shell.execute_reply.started":"2026-01-30T10:51:19.364819Z","shell.execute_reply":"2026-01-30T10:51:19.367848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loss and optimizer function\n\noptim = torch.optim.Adam(modelu.parameters(), lr=3e-5)","metadata":{"ExecuteTime":{"end_time":"2026-01-27T05:15:00.567039551Z","start_time":"2026-01-27T05:15:00.540452531Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:51:19.369539Z","iopub.execute_input":"2026-01-30T10:51:19.369903Z","iopub.status.idle":"2026-01-30T10:51:19.381757Z","shell.execute_reply.started":"2026-01-30T10:51:19.369874Z","shell.execute_reply":"2026-01-30T10:51:19.381155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-8):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, preds, targets):\n        preds = torch.sigmoid(preds)\n        intersection = (preds * targets).sum()\n        dice = 1 - (2 * intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth)\n        return dice.mean()\n        \ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"SAVING CHECK POINT......\")\n    torch.save(state, filename)\n\ndef save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n    os.makedirs(folder, exist_ok=True)\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device)\n        with torch.no_grad():\n            preds = (torch.sigmoid(model(x)) > 0.5).float()\n        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/mask_{idx}.png\")\n    model.train()\n\ndef load_checkpoint(checkpoint, model):\n    print(\"......LOADING CHECKPOINT\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\ndef save_sample_predictions(loader, model, folder=\"saved_images/\", device=\"cuda\", num_images_to_save=10, threshold=0.5):\n    os.makedirs(folder, exist_ok=True)\n    model.eval()\n\n    x, y = next(iter(loader))\n    x = x.to(device)\n    y = y.to(device)\n\n    with torch.no_grad():\n        preds = (torch.sigmoid(model(x)) > threshold).float()\n    \n    num_to_save = min(num_images_to_save, x.shape[0])\n    print(f\"Saving {num_to_save} prediction comparisons using threshold={threshold:.2f}\")\n    \n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n\ndef check_accuracy(loader, model, device=\"cuda\"):\n    dice = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            intersection = (preds * y).sum()\n            dice += (2*intersection + 1e-8) / (preds.sum() + y.sum() + 1e-8)\n    print(\"DICE SCORE:\", dice/len(loader))\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T06:18:27.085873Z","iopub.execute_input":"2026-02-03T06:18:27.086152Z","iopub.status.idle":"2026-02-03T06:18:27.095404Z","shell.execute_reply.started":"2026-02-03T06:18:27.086121Z","shell.execute_reply":"2026-02-03T06:18:27.094693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS  = 100\nfrom tqdm import tqdm\nfrom torch.amp import GradScaler,autocast\n\nscaler = GradScaler('cuda')\n\nfor epoch in range(EPOCHS):\n    modelu.train()\n    running_loss = 0.0\n\n    loop = tqdm(train_data_load, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for input_img,mask in loop:\n        mask = mask.to(device)\n        input_img = input_img.to(device)\n\n        mask = mask.squeeze(1).float()\n\n        optim.zero_grad()\n\n        with autocast('cuda'):\n            predic = modelu(input_img)\n            loss = loss_fn(predic.squeeze(1),mask)\n\n        scaler.scale(loss).backward()\n        scaler.step(optim)\n        scaler.update()\n\n        running_loss += loss.item()\n\n   \n\n    test_loss = 0.0\n\n    modelu.eval()\n    with torch.no_grad():\n        for test_img,test_mask in test_data_load:\n            test_mask = test_mask.to(device)\n            test_img = test_img.to(device)\n\n            test_mask = test_mask.squeeze(1).float()\n\n            predic_test = modelu(test_img)\n\n            tloss = loss_fn(predic_test.squeeze(1),test_mask)\n\n            test_loss += tloss.item()\n\n        avg_test_loss = test_loss/len(test_data_load)\n\n        if epoch%10 == 0:\n            print(f\" | Train Loss: {running_loss/len(train_data_load):.4f} | Test Loss: {avg_test_loss:.4f} |\")\n            \n        if ((epoch+1)%10 == 0) or ((epoch+1) == EPOCHS):\n            print(f\"\\n--- Running Validation for Epoch {epoch+1} ---\\n\")\n            \n            check_accuracy(test_data_load, modelu, device)\n            \n            save_sample_predictions(test_data_load, modelu, folder=\"saved_images\", device=device, threshold = 0.65)\n            print(\"\\n---------PREDICTIONS SAVED---------\\n\")\n        if (epoch == 24):\n            checkpoint = {\n                \"state_dict\" : modelu.state_dict(),\n                \"optimizer\" : optim.state_dict(),\n            }\n            save_checkpoint(checkpoint)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:51:19.382582Z","iopub.execute_input":"2026-01-30T10:51:19.382831Z","execution_failed":"2026-01-30T13:10:00.748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show ORIGINAL RGB image with predictions\nfrom PIL import Image\n\nmodelu.eval()\nwith torch.no_grad():\n    # Get one sample\n    test_img, test_mask = next(iter(test_data_load))\n    test_img = test_img.to(device)\n    test_mask = test_mask.to(device)\n    \n    # Get prediction\n    pred = modelu(test_img)\n    pred = torch.sigmoid(pred[10].squeeze()).cpu()\n    \n    # Get ground truth mask\n    mask = test_mask[10].cpu().squeeze()\n    \n    # Load and resize original RGB image\n    img_path = test_dataset.image_paths[10]\n    original_img = Image.open(img_path)\n    original_img = original_img.resize((512, 288))  # Resize to match (width, height)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(original_img)\n    axes[0].set_title('Original RGB Image')\n    axes[0].axis('off')\n    \n    axes[1].imshow(mask, cmap='gray')\n    axes[1].set_title('Ground Truth Mask')\n    axes[1].axis('off')\n    \n    axes[2].imshow(pred, cmap='gray')\n    axes[2].set_title('Predicted Mask')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this cell and run it BEFORE training:\nimport torch\nimport gc\n\n# Clear all GPU memory\ntorch.cuda.empty_cache()\ngc.collect()\n\n# If model already exists, delete it\ntry:\n    del modelu\n    del optim\n    del scaler\n    torch.cuda.empty_cache()\nexcept:\n    pass\n\nprint(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\nprint(f\"GPU Memory cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-30T13:10:00.749Z"}},"outputs":[],"execution_count":null}]}